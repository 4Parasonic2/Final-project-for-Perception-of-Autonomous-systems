{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d615eec3-cce8-48bf-a6d5-18a75d0a8826",
   "metadata": {},
   "source": [
    "# Final Project Sequence 3\n",
    "\n",
    "This sequence is about detecting and tracking pedestrians, cyclists and cars. In the sequence is an occlusion which the code needs to try and track when the objects goes behind that. This should be obtained based on YOLO object detection and the kalman filter we was teached about in lecture 7 about state estimation. This sequence does not have any ground truth so here we cant validate how well the model performs but here we test how good of a mdoel we obtained in sequence 2 so this is the test sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e5a06-41d4-4c3d-b4f3-b441fa816f37",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "Start by look at the raw sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad81217-7fb3-4b19-9f26-71b71dc4d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 286 images in 34759_final_project_rect/seq_03/image_02/data\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "from ultralytics import YOLO \n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Change this path if needed\n",
    "img_folder = \"34759_final_project_rect/seq_03/image_02/data\"\n",
    "\n",
    "images = sorted(glob.glob(os.path.join(img_folder, \"*.png\")))\n",
    "print(f\"Found {len(images)} images in {img_folder}\")\n",
    "\n",
    "if not images:\n",
    "    raise FileNotFoundError(\"No images found. Check your folder path.\")\n",
    "\n",
    "for i, img_path in enumerate(images):\n",
    "    frame = cv2.imread(img_path)\n",
    "    if frame is None:\n",
    "        print(\"Failed to load:\", img_path)\n",
    "        continue\n",
    "\n",
    "    cv2.imshow(\"Left Camera Sequence\", frame)\n",
    "    key = cv2.waitKey(30)\n",
    "    if key == 27:  # ESC\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168930e2-1ca7-4b68-8426-2a7e7c604bb0",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "Setup the helper functions and the kalman filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be80c903-7be9-466c-87ee-c37cec99b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the helper functions that is needed for the yolo model with the kalman filter\n",
    "def iou(box1, box2):\n",
    "    # This function is to see how much two bounding boxes is overlapping\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    # The small value is to make sure the system does not get an error of devision by 0\n",
    "    return inter_area / (union_area + 1e-9) \n",
    "\n",
    "def xyxy_to_cxcywh(box):\n",
    "    # This function turns the bounding box from corner positions into center position, width and height\n",
    "    w = box[2] - box[0]\n",
    "    h = box[3] - box[1]\n",
    "    cx = box[0] + w/2\n",
    "    cy = box[1] + h/2\n",
    "    return np.array([cx, cy, w, h])\n",
    "\n",
    "def cxcywh_to_xyxy(cx, cy, w, h):\n",
    "    # This function turns the bounding box from center position, width and height into corner positions \n",
    "    x1 = cx - w/2\n",
    "    y1 = cy - h/2\n",
    "    x2 = cx + w/2\n",
    "    y2 = cy + h/2\n",
    "    return [x1, y1, x2, y2] \n",
    "\n",
    "def associate_tracks_dets(tracks, dets_xyxy, dets_cls, iou_thresh=0.55):\n",
    "    # This functions matches existing object tracks with new detections to see if the new detections matches the existing one\n",
    "    if len(tracks) == 0:\n",
    "        return [], [], list(range(len(dets_xyxy)))\n",
    "\n",
    "    # IoU between tracktions and detections\n",
    "    iou_matrix = np.zeros((len(tracks), len(dets_xyxy)), dtype=np.float32)\n",
    "    for t, tr in enumerate(tracks):\n",
    "        t_box = tr.get_xyxy()\n",
    "        for d, d_box in enumerate(dets_xyxy):\n",
    "            if tr.cls != dets_cls[d]:\n",
    "                iou_matrix[t, d] = 0.0\n",
    "            else:\n",
    "                iou_matrix[t, d] = iou_xyxy(t_box, d_box)\n",
    "                \n",
    "    matches = []\n",
    "    unmatched_tracks = set(range(len(tracks)))\n",
    "    unmatched_dets = set(range(len(dets_xyxy)))\n",
    "\n",
    "    # Make matching based on IoU\n",
    "    if iou_matrix.size > 0:\n",
    "        indices = np.dstack(np.unravel_index(np.argsort(-iou_matrix.ravel()), iou_matrix.shape))[0]\n",
    "        for t, d in indices:\n",
    "            if iou_matrix[t, d] < iou_thresh:\n",
    "                break\n",
    "            if t in unmatched_tracks and d in unmatched_dets:\n",
    "                matches.append((t, d))\n",
    "                unmatched_tracks.remove(t)\n",
    "                unmatched_dets.remove(d)\n",
    "                \n",
    "    return matches, list(unmatched_tracks), list(unmatched_dets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a85e48a5-5bc4-452b-a3af-020a4c9f9bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KalmanBoxTracker:\n",
    "    # This class is the kalman filter for tracking the bounding boxes\n",
    "    def __init__(self, init_z, cls, dt, init_velocity=None):\n",
    "        #This function is for initializing the kalman filter\n",
    "        \n",
    "        self.dt = float(dt)\n",
    "\n",
    "        #This is the state vector\n",
    "        self.x = np.zeros((8,1), dtype=np.float32)\n",
    "        \n",
    "        self.x[0:4,0] = init_z.reshape(4) # initialize position and size\n",
    "        if init_velocity is not None:\n",
    "            self.x[4:8,0] = np.reshape(init_velocity, (4,)) # initialize velocity\n",
    "        \n",
    "        self.F = np.eye(8, dtype=np.float32) # State transition matrix\n",
    "        for i in range(4): \n",
    "            self.F[i, i+4] = self.dt\n",
    "            \n",
    "        # Measurement matrix    \n",
    "        self.H = np.zeros((4,8), dtype=np.float32)\n",
    "        self.H[0,0] = self.H[1,1] = self.H[2,2] = self.H[3,3] = 1.0\n",
    "\n",
    "        # Uncertainty Covariance matrix\n",
    "        self.P = np.eye(8, dtype=np.float32) * 20.0 \n",
    "\n",
    "        # Process noise \n",
    "        self.Q = np.eye(8, dtype=np.float32)\n",
    "        self.Q[0,0]=0.5; self.Q[1,1]=0.5 # cx, cy\n",
    "        self.Q[2,2]=0.1; self.Q[3,3]=0.1 # w, h\n",
    "        self.Q[4,4]=4.0; self.Q[5,5]=4.0 # vx, vy\n",
    "        self.Q[6,6]=1.0; self.Q[7,7]=1.0 # vm, vh\n",
    "\n",
    "        # Measurement noise\n",
    "        self.R = np.eye(4, dtype=np.float32) * 2.0\n",
    "        self.R[2,2] = 10.0 # Increased noise for width measurement\n",
    "        self.R[3,3] = 10.0 # Increased noise for height measurement\n",
    "\n",
    "        # Identity matrix\n",
    "        self.I = np.eye(8, dtype=np.float32)\n",
    "\n",
    "        # Initialization of the needed data\n",
    "        self.cls = cls # object class\n",
    "        self.id = None # ID of the tracked object\n",
    "        self.age = 0 # The number of frames since first detection\n",
    "        self.miss = 0 # The number of frames where it is not seen\n",
    "        self.score = 0.0 # The confidence of the detection\n",
    "        self.hits = 0 # The number of times where it has been hit\n",
    "        self.confirmed = False # Confirmation of the tracktion\n",
    "        self.last_detection_box = cxcywh_to_xyxy(self.x[0,0], self.x[1,0], self.x[2,0], self.x[3,0])\n",
    "        self.last_update_frame = -1\n",
    "\n",
    "    def predict(self):\n",
    "        # This function predict the next state \n",
    "        self.x = self.F @ self.x\n",
    "        self.P = self.F @ self.P @ self.F.T + self.Q\n",
    "        self.age += 1\n",
    "\n",
    "        # Reset if the detection is missed\n",
    "        if self.miss > 0:\n",
    "            self.x[6, 0] = 0.0 # vw (velocity width)\n",
    "            self.x[7, 0] = 0.0 # vh (velocity height)\n",
    "            \n",
    "        return self.get_xyxy()\n",
    "\n",
    "    def update(self, z, score, frame_idx=None):\n",
    "        # This is the update function of the kalman filter\n",
    "        z = z.reshape(4,1)\n",
    "        y = z - (self.H @ self.x)\n",
    "        S = self.H @ self.P @ self.H.T + self.R\n",
    "        K = self.P @ self.H.T @ np.linalg.inv(S)\n",
    "        self.x = self.x + (K @ y)\n",
    "        self.P = (self.I - K @ self.H) @ self.P\n",
    "        \n",
    "        self.miss = 0\n",
    "        self.score = float(score)\n",
    "        self.hits += 1\n",
    "        if self.hits >= 3:\n",
    "            self.confirmed = True\n",
    "        \n",
    "        self.last_detection_box = cxcywh_to_xyxy(float(self.x[0,0]), float(self.x[1,0]), \n",
    "                                                 float(self.x[2,0]), float(self.x[3,0]))\n",
    "        self.last_update_frame = frame_idx if frame_idx is not None else self.last_update_frame\n",
    "\n",
    "    def get_xyxy(self):\n",
    "        # This function is to get the current bounding boxes\n",
    "        cx = float(self.x[0,0]); cy = float(self.x[1,0])\n",
    "        w = max(1.0, float(self.x[2,0])); h = max(1.0, float(self.x[3,0]))\n",
    "        \n",
    "        # This is to avoid unrealistic bounding boxes\n",
    "        max_ratio = 10.0 \n",
    "        \n",
    "        if w > h * max_ratio: w = h * max_ratio\n",
    "        if h > w * max_ratio: h = w * max_ratio\n",
    "            \n",
    "        return np.array(cxcywh_to_xyxy(cx, cy, w, h), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf75c252-9e27-4d40-ab34-554a566c9de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2D tracking on 286 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking Frames: 100%|███████████████████████████████████████████████████████████████| 286/286 [02:21<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2D Tracking results saved to inference_results_seq03.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Here is the config for the object detection and tracking using both yolo and kalman filter\n",
    "IMG_FOLDER = \"34759_final_project_rect/seq_03/image_02/data\"\n",
    "OUTPUT_TRACK_FILE = \"inference_results_seq03.csv\"\n",
    "MODEL_PATH = \"yolo11l.pt\" \n",
    "\n",
    "FPS = 10.0 \n",
    "DT = 1.0 / FPS\n",
    "IOU_MATCH_THRESH = 0.55     \n",
    "MAX_INIT_PREV_DIST_SQ = 400.0**2\n",
    "SCORE_DECAY = 0.99\n",
    "DETECTION_CONF_THRESHOLD = 0.5 \n",
    "MAX_MISS_CONFIRMED = 150 \n",
    "MAX_MISS_UNCONFIRMED = 5 \n",
    "\n",
    "# Start the yolo + kalman tracking\n",
    "def run_2d_tracker():\n",
    "    try:\n",
    "        model = YOLO(MODEL_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YOLO model: {e}\")\n",
    "        return\n",
    "\n",
    "    allowed_classes = {\"person\", \"car\", \"bicycle\"}\n",
    "    tracks = []\n",
    "    next_id = 1\n",
    "    prev_frame_preds = [] \n",
    "    tracked_results = [] \n",
    "\n",
    "    images = sorted(glob.glob(os.path.join(IMG_FOLDER, \"*.png\")))\n",
    "    if not images:\n",
    "        print(f\"Error: No images found in {IMG_FOLDER}. Check path.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting 2D tracking on {len(images)} frames...\")\n",
    "\n",
    "    for frame_id, frame_path in enumerate(tqdm(images, desc=\"Tracking Frames\")):\n",
    "        \n",
    "        # Read the frame\n",
    "        frame = cv2.imread(frame_path)\n",
    "        if frame is None:\n",
    "            continue\n",
    "\n",
    "        frame_for_yolo = frame.copy() # clean input to YOLO\n",
    "        frame_disp = frame.copy() # used only for drawing \n",
    "\n",
    "        # Start the yolo detection\n",
    "        results = model(frame_for_yolo, verbose=False)\n",
    "        raw = []\n",
    "        for r in results:\n",
    "            boxes = r.boxes.xyxy.cpu().numpy() # Extracts the boundingbox coordinates into a numoy array\n",
    "            confs = r.boxes.conf.cpu().numpy() # Extracsts the confidence of the detections\n",
    "            clss = r.boxes.cls.cpu().numpy() # Extracts the class label for the detections\n",
    "\n",
    "            for box, conf, cls in zip(boxes, confs, clss):\n",
    "                label = model.names[int(cls)]\n",
    "                if label in allowed_classes and conf >= DETECTION_CONF_THRESHOLD: # Remove objects detected under a confidence of 50%\n",
    "                    raw.append((box, conf, label))\n",
    "\n",
    "        # Merge cyclists which is a person on a bike\n",
    "        used = set()\n",
    "        preds = []\n",
    "    \n",
    "        for i, (box1, conf1, label1) in enumerate(raw):\n",
    "            if label1 != \"person\":\n",
    "                continue\n",
    "    \n",
    "            for j, (box2, conf2, label2) in enumerate(raw):\n",
    "                if label2 != \"bicycle\" or j in used:\n",
    "                    continue\n",
    "    \n",
    "                if iou(box1, box2) > 0.2: #Threshold for when it is a cyclist\n",
    "                    x1 = min(int(box1[0]), int(box2[0]))\n",
    "                    y1 = min(int(box1[1]), int(box2[1]))\n",
    "                    x2 = max(int(box1[2]), int(box2[2]))\n",
    "                    y2 = max(int(box1[3]), int(box2[3]))\n",
    "                    # Add cyclists to the detections\n",
    "                    preds.append(([x1, y1, x2, y2], (conf1 + conf2)/2, \"cyclist\"))\n",
    "                    used.add(i)\n",
    "                    used.add(j)\n",
    "\n",
    "        # Add the other detections\n",
    "        for k, (box, conf, label) in enumerate(raw):\n",
    "            if k in used: continue\n",
    "            if label == \"car\":\n",
    "                preds.append((list(map(int, box)), conf, \"car\"))\n",
    "            elif label == \"person\":\n",
    "                preds.append((list(map(int, box)), conf, \"pedestrian\"))\n",
    "\n",
    "        # Start the kalman prediction\n",
    "        for tr in tracks: tr.predict()\n",
    "\n",
    "        # Match the detections to the tracking \n",
    "        dets_xyxy = [np.array(b, dtype=np.float32) for b, c, l in preds]\n",
    "        dets_cls = [l for b, c, l in preds]\n",
    "        dets_z = [xyxy_to_cxcywh(b) for b, c, l in preds]\n",
    "        matches, unmatched_tracks, unmatched_dets = associate_tracks_dets(tracks, dets_xyxy, dets_cls, IOU_MATCH_THRESH)\n",
    "\n",
    "        # Start the update with the kalman filter\n",
    "        for t, d in matches:\n",
    "            tracks[t].update(dets_z[d], preds[d][1], frame_id)\n",
    "            tracks[t].cls = dets_cls[d]\n",
    "\n",
    "        # Remove the matches that is not correct\n",
    "        for t in unmatched_tracks:\n",
    "            tr = tracks[t]\n",
    "            tr.miss += 1\n",
    "            tr.score *= SCORE_DECAY\n",
    "            \n",
    "            max_miss_allowed = MAX_MISS_CONFIRMED if tr.confirmed else MAX_MISS_UNCONFIRMED\n",
    "            \n",
    "            if tr.miss > max_miss_allowed or tr.score < 0.05:\n",
    "                tracks[t] = None\n",
    "\n",
    "        tracks = [t for t in tracks if t is not None]\n",
    "\n",
    "        # Start new tracking\n",
    "        prev_by_cls = {}\n",
    "        for p in prev_frame_preds: prev_by_cls.setdefault(p[\"cls\"], []).append(p)\n",
    "\n",
    "        for d in unmatched_dets:\n",
    "            box = dets_xyxy[d]\n",
    "            # Remove dublicates\n",
    "            if any(iou_xyxy(t.get_xyxy(), box) > 0.5 for t in tracks): continue\n",
    "            \n",
    "            init_vel = None\n",
    "            cls = dets_cls[d]\n",
    "            if cls in prev_by_cls:\n",
    "                cx, cy, _, _ = dets_z[d]\n",
    "                # Estimate the velocity\n",
    "                best = min(prev_by_cls[cls], key=lambda p: (cx-p[\"cx\"])**2 + (cy-p[\"cy\"])**2, default=None)\n",
    "                if best:\n",
    "                    d2 = (cx-best[\"cx\"])**2 + (cy-best[\"cy\"])**2\n",
    "                    if d2 < MAX_INIT_PREV_DIST_SQ:\n",
    "                        init_vel = [(cx-best[\"cx\"])/DT, (cy-best[\"cy\"])/DT, 0.0, 0.0]\n",
    "\n",
    "            # Create the new tracktion\n",
    "            tr = KalmanBoxTracker(dets_z[d], cls, DT, init_vel)\n",
    "            tr.id = next_id; next_id += 1\n",
    "            tr.score = preds[d][1]\n",
    "            tracks.append(tr)\n",
    "\n",
    "        # Remove the overlapping tracktions\n",
    "        to_rem = set()\n",
    "        for i in range(len(tracks)):\n",
    "            if i in to_rem: continue\n",
    "            for j in range(i+1, len(tracks)):\n",
    "                if j in to_rem: continue\n",
    "                if iou_xyxy(tracks[i].get_xyxy(), tracks[j].get_xyxy()) > 0.4:\n",
    "                    if tracks[i].confirmed != tracks[j].confirmed:\n",
    "                        to_rem.add(j if tracks[i].confirmed else i)\n",
    "                    else:\n",
    "                        to_rem.add(j if tracks[i].score > tracks[j].score else i)\n",
    "        tracks = [t for k, t in enumerate(tracks) if k not in to_rem]\n",
    "\n",
    "        #Start the visualization and save the data\n",
    "        for tr in tracks:\n",
    "            if not tr.confirmed and tr.miss > 0: continue\n",
    "            if tr.score < 0.5: continue \n",
    "\n",
    "            box = tr.get_xyxy().astype(int)\n",
    "            is_active = tr.miss == 0\n",
    "            \n",
    "            color = (0, 255, 0) if tr.confirmed and is_active else (255, 255, 0)\n",
    "            \n",
    "            label = f\"ID:{tr.id} {tr.cls[:3]} H:{tr.hits} S:{tr.score:.2f}\"\n",
    "\n",
    "            # Save the data\n",
    "            tracked_results.append({\n",
    "                'frame': frame_id,\n",
    "                'id': tr.id,\n",
    "                'class': tr.cls,\n",
    "                'score': tr.score,\n",
    "                'left': box[0], 'top': box[1], 'right': box[2], 'bottom': box[3],\n",
    "                'status': \"Confirmed\" if tr.confirmed else \"New\"\n",
    "            })\n",
    "\n",
    "            cv2.rectangle(frame_disp, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "            cv2.putText(frame_disp, label, (box[0], box[1]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)\n",
    "\n",
    "        prev_frame_preds = []\n",
    "        for box, conf, cls in preds:\n",
    "            z = xyxy_to_cxcywh(np.array(box))\n",
    "            prev_frame_preds.append({\"cx\": z[0], \"cy\": z[1], \"w\": z[2], \"h\": z[3], \"cls\": cls})\n",
    "\n",
    "        cv2.imshow(\"2D Tracker (YOLO Detections)\", frame_disp)\n",
    "        if cv2.waitKey(1) == 27: break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save the output\n",
    "    if tracked_results:\n",
    "        results_df = pd.DataFrame(tracked_results)\n",
    "        results_df.to_csv(OUTPUT_TRACK_FILE, index=False)\n",
    "        print(f\" 2D Tracking results saved to {OUTPUT_TRACK_FILE}.\")\n",
    "    else:\n",
    "        print(\" No detections were tracked. Output file not created.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_2d_tracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69782a0-7573-4d72-8077-e325c217f0a9",
   "metadata": {},
   "source": [
    "# Stereo vision\n",
    "\n",
    "Load the camera calibraation, compute the disparity and estimate 3D coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55392d4d-398b-457e-bde2-9b729dbe63e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focale: 707.05 px, Baseline: 0.473 m\n",
      "P_left (02):\n",
      " [[ 7.070493e+02  0.000000e+00  6.040814e+02  4.575831e+01]\n",
      " [ 0.000000e+00  7.070493e+02  1.805066e+02 -3.454157e-01]\n",
      " [ 0.000000e+00  0.000000e+00  1.000000e+00  4.981016e-03]]\n",
      "P_right (03):\n",
      " [[ 7.070493e+02  0.000000e+00  6.040814e+02 -3.341081e+02]\n",
      " [ 0.000000e+00  7.070493e+02  1.805066e+02  2.330660e+00]\n",
      " [ 0.000000e+00  0.000000e+00  1.000000e+00  3.201153e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Load files (Change paths if needed)\n",
    "left_folder = \"34759_final_project_rect/seq_03/image_02/data/\"\n",
    "right_folder = \"34759_final_project_rect/seq_03/image_03/data/\"\n",
    "calib_path = \"34759_final_project_rect/calib_cam_to_cam.txt\"\n",
    "\n",
    "def load_projection_matrices(calib_file):\n",
    "    # Load the projection from the calibration file\n",
    "    P = {}\n",
    "    with open(calib_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"P_rect_\"):\n",
    "                key, values = line.split(\":\")\n",
    "                cam = int(key.split(\"_\")[-1])\n",
    "                P[cam] = np.array(list(map(float, values.split()))).reshape(3,4)\n",
    "    return P\n",
    "\n",
    "P = load_projection_matrices(calib_path)\n",
    "P_left = P[2]   # P_rect_02\n",
    "P_right = P[3]  # P_rect_03\n",
    "\n",
    "f = P_left[0,0]               # focale in pixel\n",
    "B = abs(P_right[0,3] / f)     # baseline = Tx / f\n",
    "print(f\"Focale: {f:.2f} px, Baseline: {B:.3f} m\")\n",
    "\n",
    "np.set_printoptions(precision=15, suppress=False)\n",
    "print(\"P_left (02):\\n\", P_left)\n",
    "print(\"P_right (03):\\n\", P_right)\n",
    "\n",
    "def compute_disparity_map(left_img, right_img):\n",
    "    # Compute the disparity map between the left and right image\n",
    "    left_gray = cv2.cvtColor(left_img, cv2.COLOR_BGR2GRAY)\n",
    "    right_gray = cv2.cvtColor(right_img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # SGBM parameters\n",
    "    min_disp = 0\n",
    "    num_disp = 64 \n",
    "    block_size = 9\n",
    "    \n",
    "    stereo = cv2.StereoSGBM_create(\n",
    "        minDisparity=min_disp, \n",
    "        numDisparities=num_disp, \n",
    "        blockSize=block_size, \n",
    "        P1=8 * 3 * block_size**2,\n",
    "        P2=32 * 3 * block_size**2,\n",
    "        disp12MaxDiff=1,\n",
    "        uniquenessRatio=10,\n",
    "        speckleWindowSize=100,\n",
    "        speckleRange=32\n",
    "    )\n",
    "    \n",
    "    disparity = stereo.compute(left_gray, right_gray).astype(np.float32) / 16.0\n",
    "    disparity[disparity < 0] = np.nan \n",
    "    \n",
    "    return disparity\n",
    "\n",
    "def estimate_depth_from_disparity(disparity_map, bbox):\n",
    "    # Estimate the depth Z\n",
    "    left, top, right, bottom = [int(x) for x in bbox]\n",
    "    \n",
    "    h, w = disparity_map.shape\n",
    "    top = max(0, top); bottom = min(h, bottom)\n",
    "    left = max(0, left); right = min(w, right)\n",
    "    \n",
    "    disp_region = disparity_map[top:bottom, left:right]\n",
    "    valid_disparities = disp_region[~np.isnan(disp_region)]\n",
    "    \n",
    "    if len(valid_disparities) < 10:\n",
    "        return -1.0 \n",
    "        \n",
    "    median_disparity = np.median(valid_disparities)\n",
    "    \n",
    "    if median_disparity <= 1e-6:\n",
    "        return -1.0 \n",
    "        \n",
    "    Z_pred = BASELINE_X_FX / median_disparity\n",
    "    \n",
    "    return Z_pred\n",
    "\n",
    "def project_3d_to_2d(X_c, Y_c, Z_c, FX, FY, CX, CY):\n",
    "    #Projects a 3D point (X, Y, Z) to 2D image coordinates (u, v)\n",
    "    if Z_c <= 0: return None, None\n",
    "        \n",
    "    u = int((X_c * FX / Z_c) + CX)\n",
    "    v = int((Y_c * FY / Z_c) + CY)\n",
    "    \n",
    "    return u, v\n",
    "\n",
    "\n",
    "def calculate_pred_3d_bottom_center(track_row, Z_pred):\n",
    "    # Calculates the 3D center corresponding to the 2D bounding box bottom\n",
    "    if Z_pred <= 0:\n",
    "        return None\n",
    "        \n",
    "    # 2D bottom center\n",
    "    u_c = (track_row[\"left\"] + track_row[\"right\"]) / 2 # horizontal center\n",
    "    v_bottom = track_row[\"bottom\"]                     # vertical bottom edge\n",
    "    \n",
    "    X_pred = Z_pred * (u_c - CX) / FX\n",
    "    \n",
    "    Y_pred = Z_pred * (v_bottom - CY) / FY\n",
    "    \n",
    "    return [X_pred, Y_pred, Z_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bf4c52f-df88-415c-8322-b2075d23bf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 286 frames. Generating 3D visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Visualizing Frames: 100%|████████████████████████████████████████████████████████████| 286/286 [01:44<00:00,  2.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Path for import of images, the tracktion and the calibration\n",
    "IMG_FOLDER_LEFT = \"34759_final_project_rect/seq_03/image_02/data\"\n",
    "IMG_FOLDER_RIGHT = \"34759_final_project_rect/seq_03/image_03/data\" \n",
    "TRACK_FILE_PATH = \"inference_results_seq03.csv\" # Input from the 2D tracking script\n",
    "CALIB_FILE_PATH = \"34759_final_project_rect/calib_cam_to_cam.txt\"\n",
    "\n",
    "# Set the global parameters\n",
    "FX = 707.0493\n",
    "FY = 707.0493\n",
    "CX = 604.0814\n",
    "CY = 180.5066\n",
    "BASELINE_X_FX = 45.75831 \n",
    "\n",
    "# Visualization Colors\n",
    "COLOR_PREDICTED_BOX = (255, 0, 0)      # Blue box for 2D track\n",
    "COLOR_PRED_3D_POINT = (0, 0, 255)      # Red point for Predicted 3D center (BOTTOM)\n",
    "\n",
    "# Make the visualization with the 3D position\n",
    "def run_3d_visualization():\n",
    "    try:\n",
    "        # Load Predicted Tracks data\n",
    "        df_tracks = pd.read_csv(TRACK_FILE_PATH)\n",
    "        df_tracks = df_tracks.rename(columns={'Frame': 'frame'}) # Fix for potential case mismatch\n",
    "        tracks_grouped = df_tracks.groupby('frame')\n",
    "                \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Track file not found at {TRACK_FILE_PATH}. Run the 2D tracker first.\")\n",
    "        return\n",
    "    except KeyError:\n",
    "        print(f\"Error: Missing 'frame' or 'Frame' column in {TRACK_FILE_PATH}. Check file content.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    images_left = sorted(glob.glob(os.path.join(IMG_FOLDER_LEFT, \"*.png\")))\n",
    "    images_right = sorted(glob.glob(os.path.join(IMG_FOLDER_RIGHT, \"*.png\")))\n",
    "\n",
    "    if not images_left or len(images_left) != len(images_right):\n",
    "        print(\"Error: Image files missing or count mismatch. Check IMG_FOLDER_LEFT/RIGHT paths.\")\n",
    "        return\n",
    "\n",
    "    print(f\" Loaded {len(images_left)} frames. Generating 3D visualization...\")\n",
    "\n",
    "    for frame_id, img_path_left in enumerate(tqdm(images_left, desc=\"Visualizing Frames\")):\n",
    "        img_path_right = images_right[frame_id]\n",
    "        \n",
    "        frame_left = cv2.imread(img_path_left)\n",
    "        frame_right = cv2.imread(img_path_right)\n",
    "        \n",
    "        if frame_left is None or frame_right is None:\n",
    "            continue\n",
    "\n",
    "        frame_disp = frame_left.copy()\n",
    "        \n",
    "        # Compute Disparity Map\n",
    "        disparity_map = compute_disparity_map(frame_left, frame_right)\n",
    "\n",
    "        # Draw Predicted 2D Boxes and Predicted 3D BOTTOM Centers (Blue Box & Red Dot)\n",
    "        if frame_id in tracks_grouped.groups:\n",
    "            frame_tracks = tracks_grouped.get_group(frame_id)\n",
    "            \n",
    "            for _, track_row in frame_tracks.iterrows():\n",
    "                if not all(col in track_row for col in [\"left\", \"top\", \"right\", \"bottom\", \"id\", \"class\"]):\n",
    "                    continue\n",
    "\n",
    "                bbox = [track_row[\"left\"], track_row[\"top\"], track_row[\"right\"], track_row[\"bottom\"]]\n",
    "                track_id = int(track_row['id'])\n",
    "                cls = track_row['class']\n",
    "                \n",
    "                # Depth estimation\n",
    "                Z_pred_val = estimate_depth_from_disparity(disparity_map, bbox)\n",
    "                \n",
    "                # Estimate the 3D center corresponding to the 2D bottom edge\n",
    "                pred_3d_center = calculate_pred_3d_bottom_center(track_row, Z_pred_val)\n",
    "                \n",
    "                # Draw the 2D Bounding Box\n",
    "                box_int = [int(x) for x in bbox]\n",
    "                cv2.rectangle(frame_disp, (box_int[0], box_int[1]), (box_int[2], box_int[3]), COLOR_PREDICTED_BOX, 2)\n",
    "                \n",
    "                box_label = f\"P:{cls} ID:{track_id}\"\n",
    "                cv2.putText(frame_disp, box_label, (box_int[0], box_int[1]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_PREDICTED_BOX, 1)\n",
    "\n",
    "                # Draw the Green Predicted 3D Center (BOTTOM)\n",
    "                if pred_3d_center is not None:\n",
    "                    X_pred, Y_pred, Z_pred = pred_3d_center\n",
    "                    \n",
    "                    # Project Predicted 3D center to 2D image coordinates\n",
    "                    u_pred, v_pred = project_3d_to_2d(X_pred, Y_pred, Z_pred, FX, FY, CX, CY)\n",
    "                    \n",
    "                    if u_pred is not None:\n",
    "                        # Draw Green Predicted dot\n",
    "                        cv2.circle(frame_disp, (u_pred, v_pred), 5, COLOR_PRED_3D_POINT, -1)\n",
    "                        \n",
    "                        # Label with X, Y, Z position (in meters)\n",
    "                        pred_label_line1 = f\"3D PRED:\"\n",
    "                        pred_label_line2 = f\"X:{X_pred:.1f} Y:{Y_pred:.1f}\"\n",
    "                        pred_label_line3 = f\"Z:{Z_pred:.1f} m\"\n",
    "                        \n",
    "                        # Draw labels\n",
    "                        cv2.putText(frame_disp, pred_label_line1, (u_pred + 10, v_pred - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_PRED_3D_POINT, 1)\n",
    "                        cv2.putText(frame_disp, pred_label_line2, (u_pred + 10, v_pred - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_PRED_3D_POINT, 1)\n",
    "                        cv2.putText(frame_disp, pred_label_line3, (u_pred + 10, v_pred), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_PRED_3D_POINT, 1)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"3D Tracking Visualization\", frame_disp)\n",
    "        \n",
    "        if frame_id == 0:\n",
    "            # Show first frame for 5 seconds (5000 ms)\n",
    "            key = cv2.waitKey(30000)\n",
    "        else:\n",
    "            # Show subsequent frames for 50 ms\n",
    "            key = cv2.waitKey(50)\n",
    "        \n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_3d_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04ba8ab0-8b4d-4a58-beca-0966f3c4fb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 286 frames. Calculating 3D positions for tracked objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|█████████████████████████████████████████████████████████████| 286/286 [00:52<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3D Tracking Report saved to **3d_track_report_seq03.csv** (1651 entries).\n",
      "\n",
      "--- Example Data ---\n",
      "   frame  track_id       class     score status  left  top  right  bottom  \\\n",
      "0      0         1         car  0.935298    New     0  222    287     364   \n",
      "1      0         2         car  0.790386    New  1076  149   1197     188   \n",
      "2      0         3  pedestrian  0.789847    New   939  144    987     289   \n",
      "3      0         4  pedestrian  0.772622    New   574  149    619     308   \n",
      "4      0         5         car  0.747246    New   447  169    481     196   \n",
      "\n",
      "     X_pred    Y_pred    Z_pred  \n",
      "0 -0.505749  0.201488  0.776387  \n",
      "1  3.424266  0.048194  4.547410  \n",
      "2  0.563109  0.170216  1.109292  \n",
      "3 -0.009182  0.154405  0.856296  \n",
      "4 -1.066551  0.117964  5.383330  \n"
     ]
    }
   ],
   "source": [
    "# Path for import of images, the tracktion and the calibration\n",
    "IMG_FOLDER_LEFT = \"C:/Users/senik/Documents/2. Semester/Perception for Autonomous Systems/Final project/34759_final_project_rect/seq_03/image_02/data\"\n",
    "IMG_FOLDER_RIGHT = \"C:/Users/senik/Documents/2. Semester/Perception for Autonomous Systems/Final project/34759_final_project_rect/seq_03/image_03/data\" \n",
    "TRACK_FILE_PATH = \"inference_results_seq03.csv\" \n",
    "CALIB_FILE_PATH = \"C:/Users/senik/Documents/2. Semester/Perception for Autonomous Systems/Final project/34759_final_project_rect/calib_cam_to_cam.txt\"\n",
    "OUTPUT_REPORT_FILE = \"3d_track_report_seq03.csv\" # New output file for your report data\n",
    "\n",
    "# Function for the data generation\n",
    "def run_3d_data_generation():\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        # Load Predicted Tracks data\n",
    "        df_tracks = pd.read_csv(TRACK_FILE_PATH)\n",
    "        df_tracks = df_tracks.rename(columns={'Frame': 'frame'}) # Ensure 'frame' column name\n",
    "        tracks_grouped = df_tracks.groupby('frame')\n",
    "                \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Track file not found at {TRACK_FILE_PATH}. Run the 2D tracker first.\")\n",
    "        return\n",
    "    except KeyError:\n",
    "        print(f\"Error: Missing 'frame' or 'Frame' column in {TRACK_FILE_PATH}. Check file content.\")\n",
    "        return\n",
    "\n",
    "    images_left = sorted(glob.glob(os.path.join(IMG_FOLDER_LEFT, \"*.png\")))\n",
    "    images_right = sorted(glob.glob(os.path.join(IMG_FOLDER_RIGHT, \"*.png\")))\n",
    "\n",
    "    if not images_left or len(images_left) != len(images_right):\n",
    "        print(\"Error: Image files missing or count mismatch. Check IMG_FOLDER_LEFT/RIGHT paths.\")\n",
    "        return\n",
    "\n",
    "    print(f\" Loaded {len(images_left)} frames. Calculating 3D positions for tracked objects...\")\n",
    "\n",
    "    for frame_id, img_path_left in enumerate(tqdm(images_left, desc=\"Processing Frames\")):\n",
    "        img_path_right = images_right[frame_id]\n",
    "        \n",
    "        frame_left = cv2.imread(img_path_left)\n",
    "        frame_right = cv2.imread(img_path_right)\n",
    "        \n",
    "        if frame_left is None or frame_right is None:\n",
    "            continue\n",
    "        \n",
    "        # Compute Disparity Map once per frame\n",
    "        disparity_map = compute_disparity_map(frame_left, frame_right)\n",
    "\n",
    "        # Get Predicted tracks for this frame\n",
    "        if frame_id in tracks_grouped.groups:\n",
    "            frame_tracks = tracks_grouped.get_group(frame_id)\n",
    "            \n",
    "            # Iterate over ALL predictions in this frame\n",
    "            for _, track_row in frame_tracks.iterrows():\n",
    "                \n",
    "                # Extract 2D Bounding Box\n",
    "                bbox = [track_row[\"left\"], track_row[\"top\"], track_row[\"right\"], track_row[\"bottom\"]]\n",
    "                \n",
    "                # Estimate Depth (Z)\n",
    "                Z_pred_val = estimate_depth_from_disparity(disparity_map, bbox)\n",
    "                \n",
    "                # Calculate 3D Bottom Center (X, Y)\n",
    "                pred_3d_center = calculate_pred_3d_bottom_center(track_row, Z_pred_val)\n",
    "                \n",
    "                if pred_3d_center is not None:\n",
    "                    X_pred, Y_pred, Z_pred = pred_3d_center\n",
    "                else:\n",
    "                    X_pred, Y_pred, Z_pred = None, None, None\n",
    "\n",
    "                # Store Results\n",
    "                results.append({\n",
    "                    \"frame\": frame_id,\n",
    "                    \"track_id\": int(track_row[\"id\"]),\n",
    "                    \"class\": track_row[\"class\"],\n",
    "                    \"score\": track_row[\"score\"],\n",
    "                    \"status\": track_row[\"status\"],\n",
    "                    \"left\": track_row[\"left\"],\n",
    "                    \"top\": track_row[\"top\"],\n",
    "                    \"right\": track_row[\"right\"],\n",
    "                    \"bottom\": track_row[\"bottom\"],\n",
    "                    \"X_pred\": X_pred,\n",
    "                    \"Y_pred\": Y_pred,\n",
    "                    \"Z_pred\": Z_pred # This is the crucial depth value\n",
    "                })\n",
    "\n",
    "    # Save the output file\n",
    "    if results:\n",
    "        df_report = pd.DataFrame(results)\n",
    "        df_report.to_csv(OUTPUT_REPORT_FILE, index=False)\n",
    "        print(f\"\\n 3D Tracking Report saved to **{OUTPUT_REPORT_FILE}** ({len(df_report)} entries).\")\n",
    "        print(\"\\n--- Example Data ---\")\n",
    "        print(df_report.head())\n",
    "    else:\n",
    "        print(\"\\n No predicted tracks were found or processed. Report file not created.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_3d_data_generation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
